"""
Main AI Chat Cog for Izumi
Handles AI conversations, integrates learning engine and context builder
"""

import discord
from discord.ext import commands, tasks
from discord import app_commands
import google.generativeai as genai
import os
import time
import asyncio
import random
from typing import Dict, Optional

from .learning_engine import LearningEngine
from .context_builder import ContextBuilder
from utils.helpers import save_json, load_json

class IzumiAI(commands.Cog):
    """Main AI chat system with advanced learning capabilities"""
    
    def __init__(self, bot):
        self.bot = bot
        # Use the unified memory system instead of separate learning engine
        self.learning_engine = self.bot.unified_memory
        self.context_builder = ContextBuilder(bot, self.learning_engine)
        
        # Initialize Gemini AI
        self.gemini_model = None
        self.gemini_chat_sessions = {}  # {channel_id: session_data} - Shared per channel
        self.chat_history_limit = 100  # Messages per channel - reduced to prevent token buildup
        
        # Track Izumi's conversation participation
        self.participation_tracker = {}  # {channel_id: {"last_participation": timestamp, "is_active": bool}}
        
        # Track recent responses to prevent duplicates
        self.recent_responses = {}  # {user_id: timestamp} - Track last response time per user
        
        # API optimization features - Load persistent data
        self.response_cache = {}  # Simple response caching
        self.api_usage_data = self._load_api_usage_data()
        self.daily_api_calls = self.api_usage_data.get('daily_api_calls', 0)
        self.daily_quick_responses = self.api_usage_data.get('daily_quick_responses', 0)
        self.last_cache_clear = self.api_usage_data.get('last_cache_clear', time.time())
        
        # Detailed tracking
        self.api_call_log = self.api_usage_data.get('api_call_log', [])  # Store recent API calls for analysis
        
        # Python-based response patterns to reduce API calls
        self.quick_responses = self._init_quick_responses()
        
        # Setup Gemini
        self._setup_gemini()
        
        # Start background tasks
        self.save_learning_data_task.start()
        self.cleanup_learning_data_task.start()
    
    def _init_quick_responses(self):
        """Initialize Python-based quick responses to reduce API calls"""
        return {
            # Greetings (exact matches, case insensitive)
            'greetings': {
                'patterns': ['hi', 'hello', 'hey', 'sup', 'yo', 'hiya', 'heya'],
                'responses': [
                    'hey there!',
                    'hiya~',
                    'hello! âœ¨',
                    'hey hey!',
                    'hi! how\'s it going?',
                    'heyyy',
                    'wassup!',
                    'oh hai there'
                ]
            },
            
            # Simple questions
            'how_are_you': {
                'patterns': ['how are you', 'how r u', 'how are u', 'how you doing', 'how ya doing'],
                'responses': [
                    'doing great! thanks for asking ^^',
                    'pretty good! just vibing~',
                    'i\'m doing well! how about you?',
                    'all good here! what about you?',
                    'doing amazing! hope you are too',
                    'pretty chill today, thanks!',
                    'good! just hanging out in here'
                ]
            },
            
            # Thanks
            'thanks': {
                'patterns': ['thanks', 'thank you', 'thx', 'ty', 'tysm', 'thank u'],
                'responses': [
                    'you\'re welcome!',
                    'no problem!',
                    'anytime! ^^',
                    'happy to help!',
                    'of course!',
                    'np!',
                    'always here to help~'
                ]
            },
            
            # Simple affirmations
            'yes_no': {
                'patterns': ['yes', 'yeah', 'yep', 'yup', 'no', 'nope', 'nah'],
                'responses': [
                    'gotcha!',
                    'alright!',
                    'okay!',
                    'sounds good',
                    'cool cool',
                    'fair enough',
                    'makes sense'
                ]
            },
            
            # Good night/morning
            'time_greetings': {
                'patterns': ['good morning', 'good night', 'goodnight', 'gn', 'gm', 'good evening'],
                'responses': [
                    'good morning!',
                    'good night! sleep well~',
                    'sweet dreams!',
                    'have a great day!',
                    'good evening!',
                    'night night!',
                    'morning! hope you have a good day'
                ]
            },
            
            # Laughter responses
            'laughter': {
                'patterns': ['lol', 'lmao', 'haha', 'lmfao', 'rofl', 'xd'],
                'responses': [
                    'hehe',
                    'ikr!',
                    'lmao',
                    'haha nice',
                    'that\'s funny',
                    'lol',
                    'ðŸ˜„'
                ]
            }
        }
    
    def _load_api_usage_data(self) -> dict:
        """Load persistent API usage data from file"""
        try:
            from utils.helpers import load_json
            data = load_json('data/api_usage_data.json')
            
            # Check if data is from today (reset daily counters if new day)
            import datetime
            current_date = datetime.datetime.now().strftime('%Y-%m-%d')
            stored_date = data.get('date', '')
            
            if stored_date != current_date:
                # New day, reset daily counters but keep historical data
                print(f"ðŸ”„ New day detected ({stored_date} â†’ {current_date}). Resetting daily counters.")
                return {
                    'date': current_date,
                    'daily_api_calls': 0,
                    'daily_quick_responses': 0,
                    'last_cache_clear': time.time(),
                    'api_call_log': [],
                    'total_api_calls': data.get('total_api_calls', 0),
                    'total_quick_responses': data.get('total_quick_responses', 0),
                    'historical_data': data.get('historical_data', [])
                }
            
            print(f"ðŸ“Š Loaded API usage data: {data.get('daily_api_calls', 0)} API calls today")
            return data
            
        except FileNotFoundError:
            print("ðŸ“ No existing API usage data found, starting fresh.")
            import datetime
            return {
                'date': datetime.datetime.now().strftime('%Y-%m-%d'),
                'daily_api_calls': 0,
                'daily_quick_responses': 0,
                'last_cache_clear': time.time(),
                'api_call_log': [],
                'total_api_calls': 0,
                'total_quick_responses': 0,
                'historical_data': []
            }
        except Exception as e:
            print(f"âš ï¸ Error loading API usage data: {e}")
            import datetime
            return {
                'date': datetime.datetime.now().strftime('%Y-%m-%d'),
                'daily_api_calls': 0,
                'daily_quick_responses': 0,
                'last_cache_clear': time.time(),
                'api_call_log': [],
                'total_api_calls': 0,
                'total_quick_responses': 0,
                'historical_data': []
            }
    
    def _save_api_usage_data(self):
        """Save persistent API usage data to file"""
        try:
            from utils.helpers import save_json
            import datetime
            
            # Update the data structure
            self.api_usage_data.update({
                'date': datetime.datetime.now().strftime('%Y-%m-%d'),
                'daily_api_calls': self.daily_api_calls,
                'daily_quick_responses': self.daily_quick_responses,
                'last_cache_clear': self.last_cache_clear,
                'api_call_log': self.api_call_log,
                'total_api_calls': self.api_usage_data.get('total_api_calls', 0) + (self.daily_api_calls - self.api_usage_data.get('daily_api_calls', 0)),
                'total_quick_responses': self.api_usage_data.get('total_quick_responses', 0) + (self.daily_quick_responses - self.api_usage_data.get('daily_quick_responses', 0))
            })
            
            save_json('data/api_usage_data.json', self.api_usage_data)
            
        except Exception as e:
            print(f"âš ï¸ Error saving API usage data: {e}")

    def _check_quick_response(self, message_content: str) -> str:
        """Check if we can use a quick Python response instead of API call"""
        content_lower = message_content.lower().strip()
        
        # Remove common punctuation for matching
        content_clean = content_lower.replace('!', '').replace('?', '').replace('.', '').replace(',', '')
        
        for category, data in self.quick_responses.items():
            for pattern in data['patterns']:
                if pattern == content_clean or (len(content_clean.split()) <= 3 and pattern in content_clean):
                    # Use mood to influence response selection
                    try:
                        mood_data = self.bot.unified_memory.get_daily_mood()
                        current_mood = mood_data['current_mood']
                        
                        # Filter responses based on mood
                        responses = data['responses']
                        if current_mood == 'sleepy':
                            # Prefer shorter, more casual responses when sleepy
                            responses = [r for r in responses if len(r) < 15] or responses
                        elif current_mood == 'excited':
                            # Prefer more energetic responses when excited
                            responses = [r for r in responses if '!' in r or '~' in r] or responses
                        
                        return random.choice(responses)
                    except:
                        return random.choice(data['responses'])
        
        return None
    
    def _setup_gemini(self):
        """Initialize Gemini AI model"""
        GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
        
        if not GEMINI_API_KEY:
            print("Warning: GEMINI_API_KEY not found. AI chat features will be disabled.")
            return
        
        genai.configure(api_key=GEMINI_API_KEY)
        
        # Safety settings
        self.safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        # Model hierarchy (best â†’ fallback)
        MODEL_HIERARCHY = [
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite", 
            "gemini-2.5-pro",
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite",
            "gemini-1.5-flash",
            "gemini-1.5-flash-8b",
        ]
        
        # Try to initialize model
        for model_name in MODEL_HIERARCHY:
            try:
                self.gemini_model = genai.GenerativeModel(
                    model_name=model_name,
                    safety_settings=self.safety_settings,
                    system_instruction=self.bot.system_prompt
                )
                print(f"Gemini AI model initialized with: {model_name}")
                break
            except Exception as e:
                print(f"Failed to init {model_name}, trying next... ({e})")
        
        if not self.gemini_model:
            print("âš ï¸ No Gemini models available.")
    
    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        """Handle all messages for learning and AI responses"""
        if message.author.bot or not message.guild:
            return
        
        # ALWAYS learn from messages (even when not mentioned)
        await self.learning_engine.learn_from_message(message)
        
        # Check for song lyrics first (before other responses)
        lyrics_match = await self._check_for_lyrics(message)
        if lyrics_match:
            return  # Lyrics response sent, don't process further
        
        # Handle AI responses when mentioned
        if self.bot.user in message.mentions:
            print(f"ðŸ¤– {message.author.display_name} mentioned Izumi: {message.content[:100]}...")
            await self._handle_ai_response(message)
        else:
            # Check if someone mentioned "izumi" and she recently participated
            if await self._should_continue_conversation(message):
                await self._continue_conversation(message)
            else:
                # Check for conversation participation opportunity
                await self._check_conversation_participation(message)

    @commands.Cog.listener()
    async def on_message_edit(self, before: discord.Message, after: discord.Message):
        """Handle message edits - respond to corrected messages that mention Izumi"""
        if after.author.bot or not after.guild:
            return
        
        # Only respond to edits if:
        # 1. The edited message now mentions Izumi (but didn't before)
        # 2. OR the message mentioned Izumi before but content significantly changed
        
        before_mentions_izumi = self.bot.user in before.mentions
        after_mentions_izumi = self.bot.user in after.mentions
        
        # Case 1: Now mentions Izumi (but didn't before)
        if after_mentions_izumi and not before_mentions_izumi:
            print(f"ðŸ“ Message edited to mention Izumi: {after.content[:50]}...")
            await self._handle_ai_response(after)
            return
        
        # Case 2: Already mentioned Izumi, but content changed significantly
        if before_mentions_izumi and after_mentions_izumi:
            # Check if the content changed significantly (more than just typo fixes)
            before_clean = before.content.replace(f'<@{self.bot.user.id}>', '').replace(f'<@!{self.bot.user.id}>', '').strip()
            after_clean = after.content.replace(f'<@{self.bot.user.id}>', '').replace(f'<@!{self.bot.user.id}>', '').strip()
            
            # Simple similarity check - if less than 70% similar, respond to the edit
            similarity = self._calculate_text_similarity(before_clean, after_clean)
            if similarity < 0.7:
                print(f"ðŸ“ Significant edit detected (similarity: {similarity:.2f}): {after.content[:50]}...")
                # Add a small delay to avoid instant responses to edits
                await asyncio.sleep(2)
                await self._handle_ai_response(after)
            else:
                print(f"ðŸ“ Minor edit detected (similarity: {similarity:.2f}), ignoring")

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """Calculate simple text similarity between two strings"""
        if not text1 or not text2:
            return 0.0
        
        # Simple word-based similarity
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    async def _check_for_lyrics(self, message: discord.Message) -> bool:
        """Check if message contains song lyrics and respond with continuation"""
        # Get lyrics cog
        lyrics_cog = self.bot.get_cog('Lyrics')
        if not lyrics_cog:
            return False
        
        # Check if message matches any known lyrics
        match = lyrics_cog.find_matching_lyric(message.content)
        
        if match and match['confidence'] >= 0.75:  # High confidence match
            print(f"ðŸŽµ Detected lyrics from '{match['song']}' by {match['artist']}")
            
            # Random chance to respond (70% chance to make it feel natural, not every time)
            if random.randint(1, 100) <= 70:
                # Add typing delay for natural feel
                await self._type_with_delay(message.channel, match['next_line'])
                
                # Send the next line
                await message.channel.send(match['next_line'])
                
                # Occasionally add a comment about the song (20% chance)
                if random.randint(1, 100) <= 20:
                    comments = [
                        f"love that song! ðŸŽµ",
                        f"*vibes to {match['song']}* âœ¨",
                        f"omg {match['artist']} is so good",
                        f"this song is such a bop",
                        f"*continues singing*",
                        f"great taste in music!"
                    ]
                    await asyncio.sleep(1.5)
                    await message.channel.send(random.choice(comments))
                
                return True
        
        return False
    
    async def _check_conversation_participation(self, message: discord.Message):
        """Check if Izumi should join an active conversation"""
        if not self.gemini_model:
            return
        
        conversation_analysis = self.learning_engine.detect_active_conversation(message.channel.id)
        
        if conversation_analysis["should_participate"]:
            print(f"ðŸŽ­ Joining conversation in #{message.channel.name} - {conversation_analysis['message_count']} messages from {len(conversation_analysis['participants'])} users")
            
            # Generate contextual response to join the conversation
            context = conversation_analysis["conversation_context"]
            
            # Add a small delay to make it feel more natural
            import asyncio
            await asyncio.sleep(2)
            
            try:
                # Generate response with conversation context
                response_text = await self._generate_response_with_fallback(
                    user_id=message.author.id,
                    channel_id=message.channel.id,
                    prompt=context,
                    original_message="[joining conversation]",
                    username=message.author.display_name
                )
                
                if response_text and len(response_text.strip()) > 0:
                    # Split response into multiple messages if needed
                    message_parts = self._split_response_naturally(response_text)
                    print(f"ðŸ”§ Join conversation split into {len(message_parts)} parts: {[len(part) for part in message_parts]}")
                    
                    # Send each part with appropriate delays
                    for i, part in enumerate(message_parts):
                        print(f"ðŸ”§ Sending join part {i+1}/{len(message_parts)}: {part[:30]}...")
                        
                        # Show typing indicator with human-like delay
                        await self._type_with_delay(message.channel, part)
                        await message.channel.send(part)
                        
                        # Brief pause between multiple messages (if more than one part)
                        if len(message_parts) > 1 and i < len(message_parts) - 1:
                            pause_duration = random.uniform(2.0, 3.0)
                            print(f"ðŸ”§ Pausing {pause_duration:.1f}s between join messages...")
                            await asyncio.sleep(pause_duration)
                    
                    print(f"âœ… Successfully joined conversation: {response_text[:50]}...")
                    
                    # Track that Izumi is now participating in this conversation
                    self.participation_tracker[message.channel.id] = {
                        "last_participation": time.time(),
                        "is_active": True
                    }
                
            except Exception as e:
                print(f"âŒ Error joining conversation: {e}")

    async def _should_continue_conversation(self, message: discord.Message) -> bool:
        """Check if Izumi should continue participating in a conversation"""
        channel_id = message.channel.id
        
        # Check if we're tracking this channel
        if channel_id not in self.participation_tracker:
            return False
        
        tracker = self.participation_tracker[channel_id]
        
        # Check if Izumi is currently active in this conversation
        if not tracker.get("is_active", False):
            return False
        
        # Check if it's been less than 60 seconds since last participation
        time_since_last = time.time() - tracker.get("last_participation", 0)
        if time_since_last > 60:
            # Reset participation status if too much time has passed
            tracker["is_active"] = False
            return False
        
        # Check if message contains "izumi" (case insensitive)
        message_lower = message.content.lower()
        if "izumi" in message_lower:
            print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - 'izumi' mentioned within 60s window")
            return True
        
        # Additional smart continuation triggers
        return await self._check_smart_continuation_triggers(message, tracker, time_since_last)

    async def _check_smart_continuation_triggers(self, message: discord.Message, tracker: dict, time_since_last: float) -> bool:
        """Check for intelligent conversation continuation triggers"""
        message_lower = message.content.lower()
        
        # 1. Direct questions to the group (30% chance within 30 seconds)
        question_words = ["what do you", "what does everyone", "anyone know", "does anyone", "what should", "which", "who thinks", "thoughts?", "opinions?"]
        if any(word in message_lower for word in question_words) and time_since_last <= 30:
            if self._random_chance(30):  # 30% chance
                print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - question detected, 30% trigger")
                return True
        
        # 2. Controversial/opinion topics (20% chance within 45 seconds)
        opinion_triggers = ["i think", "i believe", "in my opinion", "personally", "i disagree", "i agree", "hot take", "unpopular opinion", "change my mind"]
        if any(trigger in message_lower for trigger in opinion_triggers) and time_since_last <= 45:
            if self._random_chance(20):  # 20% chance
                print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - opinion topic detected, 20% trigger")
                return True
        
        # 3. When someone asks for help/advice (40% chance within 40 seconds)
        help_triggers = ["help", "how do i", "can someone", "need advice", "what's the best", "recommendations", "suggestions"]
        if any(trigger in message_lower for trigger in help_triggers) and time_since_last <= 40:
            if self._random_chance(40):  # 40% chance
                print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - help request detected, 40% trigger")
                return True
        
        # 4. Emotional content (25% chance within 35 seconds)
        emotional_triggers = ["excited", "sad", "angry", "frustrated", "happy", "worried", "stressed", "amazing", "terrible", "love", "hate"]
        if any(trigger in message_lower for trigger in emotional_triggers) and time_since_last <= 35:
            if self._random_chance(25):  # 25% chance
                print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - emotional content detected, 25% trigger")
                return True
        
        # 5. Follow-up to Izumi's last message (50% chance within 20 seconds)
        # This triggers when someone responds shortly after Izumi's message
        if time_since_last <= 20:
            # Check if this might be a response to Izumi
            response_indicators = ["yeah", "true", "exactly", "i agree", "disagree", "but", "however", "also", "plus", "additionally"]
            if any(indicator in message_lower for indicator in response_indicators):
                if self._random_chance(50):  # 50% chance
                    print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - follow-up response detected, 50% trigger")
                    return True
        
        # 6. Gaming/tech topics that Izumi might be interested in (15% chance within 50 seconds)
        interest_triggers = ["osu", "anime", "gaming", "code", "programming", "discord", "bot", "ai", "technology", "computer"]
        if any(trigger in message_lower for trigger in interest_triggers) and time_since_last <= 50:
            if self._random_chance(15):  # 15% chance
                print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - interest topic detected, 15% trigger")
                return True
        
        # 7. Conversation lull - proactively continue (10% chance if 15-25 seconds of silence)
        if 15 <= time_since_last <= 25:
            if self._random_chance(10):  # 10% chance to revive conversation
                print(f"ðŸŽ­ Continuing conversation in #{message.channel.name} - conversation lull detected, 10% trigger")
                return True
        
        return False

    def _random_chance(self, percentage: int) -> bool:
        """Return True based on percentage chance"""
        import random
        return random.randint(1, 100) <= percentage

    async def _continue_conversation(self, message: discord.Message):
        """Continue participating in an ongoing conversation"""
        if not self.gemini_model:
            return
        
        try:
            # Build context from recent conversation
            context = self.context_builder.build_smart_context(
                user_id=message.author.id,
                guild_id=message.guild.id,
                current_message=message.content,
                channel_id=message.channel.id
            )
            
            # Determine the type of continuation and create appropriate prompt
            continuation_type = self._get_continuation_type(message)
            conversation_prompt = self._build_continuation_prompt(context, message, continuation_type)
            
            # Generate response
            response_text = await self._generate_response_with_fallback(
                user_id=message.author.id,
                channel_id=message.channel.id,
                prompt=conversation_prompt,
                original_message=message.content,
                username=message.author.display_name
            )
            
            if response_text and len(response_text.strip()) > 0:
                # Split response into multiple messages if needed
                message_parts = self._split_response_naturally(response_text)
                print(f"ðŸ”§ Continuation split into {len(message_parts)} parts: {[len(part) for part in message_parts]}")
                
                # Send each part with appropriate delays
                for i, part in enumerate(message_parts):
                    print(f"ðŸ”§ Sending continuation part {i+1}/{len(message_parts)}: {part[:30]}...")
                    
                    # Show typing indicator with human-like delay
                    await self._type_with_delay(message.channel, part)
                    await message.channel.send(part)
                    
                    # Brief pause between multiple messages (if more than one part)
                    if len(message_parts) > 1 and i < len(message_parts) - 1:
                        pause_duration = random.uniform(2.0, 3.0)
                        print(f"ðŸ”§ Pausing {pause_duration:.1f}s between continuation messages...")
                        await asyncio.sleep(pause_duration)
                
                print(f"âœ… Continued conversation: {response_text[:50]}...")
                
                # Update participation tracker
                self.participation_tracker[message.channel.id]["last_participation"] = time.time()
            
        except Exception as e:
            print(f"âŒ Error continuing conversation: {e}")

    def _get_continuation_type(self, message: discord.Message) -> str:
        """Determine what type of continuation this is"""
        message_lower = message.content.lower()
        
        if "izumi" in message_lower:
            return "name_mention"
        
        question_words = ["what do you", "what does everyone", "anyone know", "does anyone", "what should", "which", "who thinks", "thoughts?", "opinions?"]
        if any(word in message_lower for word in question_words):
            return "question"
        
        opinion_triggers = ["i think", "i believe", "in my opinion", "personally", "i disagree", "i agree", "hot take", "unpopular opinion"]
        if any(trigger in message_lower for trigger in opinion_triggers):
            return "opinion"
        
        help_triggers = ["help", "how do i", "can someone", "need advice", "what's the best", "recommendations", "suggestions"]
        if any(trigger in message_lower for trigger in help_triggers):
            return "help_request"
        
        emotional_triggers = ["excited", "sad", "angry", "frustrated", "happy", "worried", "stressed", "amazing", "terrible", "love", "hate"]
        if any(trigger in message_lower for trigger in emotional_triggers):
            return "emotional"
        
        response_indicators = ["yeah", "true", "exactly", "i agree", "disagree", "but", "however", "also", "plus", "additionally"]
        if any(indicator in message_lower for indicator in response_indicators):
            return "follow_up"
        
        interest_triggers = ["osu", "anime", "gaming", "code", "programming", "discord", "bot", "ai", "technology", "computer"]
        if any(trigger in message_lower for trigger in interest_triggers):
            return "interest_topic"
        
        # Check if this is a conversation lull (based on timing rather than content)
        channel_id = message.channel.id
        if channel_id in self.participation_tracker:
            time_since_last = time.time() - self.participation_tracker[channel_id].get("last_participation", 0)
            if 15 <= time_since_last <= 25:
                return "conversation_lull"
        
        return "general"

    def _build_continuation_prompt(self, context: str, message: discord.Message, continuation_type: str) -> str:
        """Build an appropriate prompt based on continuation type"""
        base_prompt = f"{context}\n\nUser message: {message.content}\n\n"
        
        instructions = {
            "name_mention": "Someone mentioned your name in the conversation. Respond naturally and appropriately to what they said.",
            
            "question": "Someone asked a question to the group. You can chime in with your thoughts or knowledge if you have something valuable to add. Keep it conversational and helpful.",
            
            "opinion": "Someone shared their opinion or belief. You can agree, disagree, or add your own perspective. Be respectful but feel free to have your own thoughts.",
            
            "help_request": "Someone is asking for help or advice. If you have useful knowledge or suggestions, share them in a helpful and supportive way.",
            
            "emotional": "Someone expressed emotions or feelings. Respond with appropriate empathy and support, or share in their excitement if it's positive.",
            
            "follow_up": "Someone seems to be responding or following up on the conversation. Continue the natural flow of discussion.",
            
            "interest_topic": "Someone mentioned a topic you might be interested in (gaming, tech, anime, etc.). Feel free to join in with your own thoughts or experiences.",
            
            "conversation_lull": "The conversation seems to be slowing down. You can ask a related question, make an observation, or share a thought to keep the discussion going naturally.",
            
            "general": "Continue participating in this ongoing conversation naturally."
        }
        
        instruction = instructions.get(continuation_type, instructions["general"])
        return f"{base_prompt}Instruction: {instruction}"
    
    async def _handle_ai_response(self, message: discord.Message):
        """Generate and send AI response with delay to catch follow-up messages"""
        if not self.gemini_model:
            return
        
        # Check if we recently responded to this user (prevent spam responses)
        user_id = message.author.id
        current_time = time.time()
        
        if user_id in self.recent_responses:
            time_since_last_response = current_time - self.recent_responses[user_id]
            if time_since_last_response < 15:  # Minimum 15 seconds between responses to same user
                print(f"â³ Skipping response to {message.author.display_name} - responded {time_since_last_response:.1f}s ago")
                return
        
        # Mark that we're about to respond to this user
        self.recent_responses[user_id] = current_time

        # Wait 4 seconds to collect any follow-up messages from the same user
        await asyncio.sleep(4)
        
        # Collect recent messages from the same user in the last 10 seconds
        recent_messages = await self._collect_recent_user_messages(message)
        
        # Check for emotional responses first (for users returning after absence)
        emotional_context = self.learning_engine.get_emotional_context(message.author.id, message.guild.id)
        
        # For strong emotional situations, use pre-generated response
        if emotional_context["type"] in ["completely_absent", "being_ignored"] and emotional_context.get("message"):
            await message.reply(emotional_context["message"])
            return
        
        try:
            # Combine all recent messages from the user
            combined_prompt = self._combine_user_messages(recent_messages)
            print(f"ðŸ”§ Combined conversation context:\n{combined_prompt}")
            
            if not combined_prompt:
                combined_prompt = "hello"
            
            # Check for quick Python responses first (saves API calls)
            quick_response = self._check_quick_response(combined_prompt)
            if quick_response:
                self.daily_quick_responses += 1
                self._save_api_usage_data()  # Save persistent data
                print(f"ðŸš€ Using quick response #{self.daily_quick_responses} (API saved): {quick_response}")
                
                # Split response if needed and send
                message_parts = self._split_response_naturally(quick_response)
                
                for i, part in enumerate(message_parts):
                    await self._type_with_delay(message.channel, part)
                    
                    if i == 0:
                        await message.reply(part, mention_author=False)
                    else:
                        await message.channel.send(part)
                    
                    # Brief pause between multiple messages (if more than one part)
                    if len(message_parts) > 1 and i < len(message_parts) - 1:
                        await asyncio.sleep(random.uniform(2.0, 3.0))
                
                # Track participation
                self.participation_tracker[message.channel.id] = {
                    "last_participation": time.time(),
                    "is_active": True
                }
                return
            
            # Process mentions for AI context
            processed_prompt = self.bot.process_mentions_for_ai(combined_prompt, message.guild.id)
            
            # Build comprehensive context
            context = self.context_builder.build_smart_context(
                user_id=message.author.id,
                guild_id=message.guild.id,
                current_message=processed_prompt,
                channel_id=message.channel.id
            )
            
            # Create full prompt with proper conversation context labeling
            if len(recent_messages) > 1:
                full_prompt = f"{context}\n\nRecent conversation:\n{processed_prompt}\n\nPlease respond to the full conversation context above."
            else:
                full_prompt = f"{context}\n\nUser message: {processed_prompt}"
            
            # Generate response with fallback system
            response_text = await self._generate_response_with_fallback(
                user_id=message.author.id,
                channel_id=message.channel.id,
                prompt=full_prompt,
                original_message=processed_prompt,
                username=message.author.display_name
            )
            
            if not response_text:
                response_text = "sorry, having technical issues rn"
            
            # Split response into multiple messages if needed
            # First, handle any newlines in the response by splitting on them
            response_text = self._preprocess_response_for_splitting(response_text)
            message_parts = self._split_response_naturally(response_text)
            print(f"ðŸ”§ Split into {len(message_parts)} parts: {[len(part) for part in message_parts]}")
            
            # Send each part with appropriate delays
            for i, part in enumerate(message_parts):
                print(f"ðŸ”§ Sending part {i+1}/{len(message_parts)}: {part[:30]}...")
                
                # Show typing indicator with human-like delay
                await self._type_with_delay(message.channel, part)
                
                if i == 0:
                    # Reply to the original message for the first part
                    await message.reply(part, mention_author=False)
                else:
                    # Send follow-up messages normally
                    await message.channel.send(part)
                
                # Brief pause between multiple messages (if more than one part)
                if len(message_parts) > 1 and i < len(message_parts) - 1:
                    pause_duration = random.uniform(2.0, 3.0)
                    print(f"ðŸ”§ Pausing {pause_duration:.1f}s between messages...")
                    await asyncio.sleep(pause_duration)
            
            # Track that Izumi is now participating in this conversation
            self.participation_tracker[message.channel.id] = {
                "last_participation": time.time(),
                "is_active": True
            }
            
        except Exception as e:
            print(f"Error in AI response: {e}")
            await message.reply("sorry, having technical issues rn", mention_author=False)

    async def _collect_recent_user_messages(self, original_message: discord.Message) -> list:
        """Collect recent conversation context from all users within 30 seconds"""
        messages = []
        
        try:
            # Get recent conversation context from ALL users, not just the original author
            async for msg in original_message.channel.history(limit=15, before=original_message.created_at):
                # Collect messages from anyone within 30 seconds for better conversation context
                time_diff = (original_message.created_at - msg.created_at).total_seconds()
                if time_diff <= 30 and not msg.author.bot:  # Don't include bot messages
                    messages.append(msg)
                elif time_diff > 30:
                    break
            
            # Add the original message
            messages.append(original_message)
            
            # Also get any messages that came AFTER (during the processing delay)
            async for msg in original_message.channel.history(limit=5, after=original_message.created_at):
                # Only collect messages from the original user within 10 seconds (for multi-part messages)
                time_diff = (msg.created_at - original_message.created_at).total_seconds()
                if msg.author == original_message.author and time_diff <= 10:
                    messages.append(msg)
                elif time_diff > 10:
                    break
                    
        except Exception as e:
            print(f"Error collecting recent messages: {e}")
            return [original_message]
        
        # Sort by timestamp to maintain order
        messages.sort(key=lambda m: m.created_at)
        print(f"ðŸ“¥ Collected {len(messages)} messages from conversation context (last 30s)")
        return messages

    def _combine_user_messages(self, messages: list) -> str:
        """Combine conversation context from multiple users into a natural prompt"""
        if not messages:
            return ""
        
        combined_parts = []
        for msg in messages:
            # Remove bot mentions from each message
            content = msg.content.replace(f'<@{self.bot.user.id}>', '').strip()
            content = content.replace(f'<@!{self.bot.user.id}>', '').strip()
            if content:
                # Include who said what for better context
                author_name = msg.author.display_name
                combined_parts.append(f"{author_name}: {content}")
        
        return '\n'.join(combined_parts)

    def _preprocess_response_for_splitting(self, response: str) -> str:
        """Preprocess response to handle newlines and prepare for message splitting"""
        if not response:
            return response
        
        # If the response contains newlines, it might be intended as separate messages
        # Replace newlines with a special marker that our splitting function can handle
        
        # First, clean up any double newlines or excessive whitespace
        import re
        response = re.sub(r'\n\s*\n', '\n', response)  # Remove double newlines
        response = re.sub(r'\n\s+', '\n', response)    # Remove whitespace after newlines
        
        # If there are newlines, split on them and treat each line as a separate message intention
        if '\n' in response:
            lines = [line.strip() for line in response.split('\n') if line.strip()]
            if len(lines) > 1:
                # Join with a special separator that our splitting function will recognize
                return ' |SPLIT| '.join(lines)
        
        return response

    def _split_response_naturally(self, response: str) -> list:
        """Split response at natural speech boundaries - complete sentences and natural pauses"""
        import re
        
        # Debug logging
        print(f"ðŸ”§ Splitting response (length: {len(response)}): {response[:100]}...")
        
        # First check if the response contains our special split marker (from newlines)
        if '|SPLIT|' in response:
            pre_split_parts = [part.strip() for part in response.split('|SPLIT|') if part.strip()]
            print(f"ðŸ”§ Found {len(pre_split_parts)} pre-split parts from newlines")
            
            # Each newline-separated part should be its own message
            final_parts = []
            for part in pre_split_parts:
                # Only split further if the part is extremely long (over 150 chars)
                if len(part) > 150:
                    sub_parts = self._split_long_text_at_sentences(part)
                    final_parts.extend(sub_parts)
                else:
                    final_parts.append(part)
            return final_parts
        
        # For regular responses, split at sentence boundaries
        return self._split_long_text_at_sentences(response)
    
    def _split_long_text_at_sentences(self, text: str) -> list:
        """Split text at natural sentence boundaries and speech pauses"""
        import re
        
        text = text.strip()
        if not text:
            return []
        
        # Keep short messages intact (under 100 characters)
        if len(text) <= 100:
            print(f"ðŸ”§ Keeping short message intact")
            return [text]
        
        parts = []
        
        # Primary split points (complete sentences)
        # Split on sentence endings followed by space/start of new sentence
        sentence_pattern = r'([.!?]+)\s+(?=[A-Z]|[a-z])'
        sentences = re.split(sentence_pattern, text)
        
        # Recombine sentences with their punctuation
        current_part = ""
        for i in range(0, len(sentences), 2):
            sentence = sentences[i]
            punctuation = sentences[i + 1] if i + 1 < len(sentences) else ""
            
            full_sentence = sentence + punctuation
            
            # If adding this sentence would make the part too long, save current part
            if current_part and len(current_part + " " + full_sentence) > 120:
                if current_part.strip():
                    parts.append(current_part.strip())
                current_part = full_sentence
            else:
                # Add to current part
                if current_part:
                    current_part += " " + full_sentence
                else:
                    current_part = full_sentence
        
        # Add any remaining text
        if current_part.strip():
            parts.append(current_part.strip())
        
        # If we didn't get good sentence splits (like one very long sentence), 
        # fall back to natural pause points
        if len(parts) == 1 and len(parts[0]) > 120:
            parts = self._split_at_natural_pauses(parts[0])
        
        # Clean up and validate
        parts = [part.strip() for part in parts if part.strip()]
        
        print(f"ðŸ”§ Split into {len(parts)} natural parts at sentence boundaries")
        part_lengths = [len(part) for part in parts]
        print(f"ðŸ”§ Part lengths: {part_lengths}")
        
        return parts if parts else [text]
    
    def _split_at_natural_pauses(self, text: str) -> list:
        """Split at natural speech pauses when sentence splitting fails"""
        
        # Natural pause points in order of preference
        pause_points = [
            r',\s+(?=and\s)',     # ", and"
            r',\s+(?=but\s)',     # ", but" 
            r',\s+(?=so\s)',      # ", so"
            r',\s+(?=or\s)',      # ", or"
            r',\s+(?=because\s)', # ", because"
            r',\s+(?=while\s)',   # ", while"
            r',\s+(?=when\s)',    # ", when"
            r',\s+(?=if\s)',      # ", if"
            r',\s+(?=since\s)',   # ", since"
            r';\s+',              # semicolons
            r':\s+',              # colons
            r'\s+--\s+',          # em dashes
            r'\s+-\s+',           # regular dashes with spaces
            r',\s+',              # any comma with space
        ]
        
        parts = []
        remaining = text.strip()
        
        while remaining and len(remaining) > 100:
            best_split = None
            best_position = 0
            
            # Find the best pause point around the middle of remaining text
            target_position = len(remaining) // 2
            
            for pattern in pause_points:
                import re
                matches = list(re.finditer(pattern, remaining))
                
                for match in matches:
                    position = match.end()
                    # Prefer splits closer to the middle, but not too early or late
                    if 50 <= position <= len(remaining) - 30:
                        distance_from_target = abs(position - target_position)
                        if best_split is None or distance_from_target < abs(best_position - target_position):
                            best_split = pattern
                            best_position = position
                
                # If we found a good split with this pattern, use it
                if best_split:
                    break
            
            if best_split:
                part = remaining[:best_position].strip()
                if part:
                    parts.append(part)
                remaining = remaining[best_position:].strip()
            else:
                # No good pause point found, just break
                break
        
        # Add any remaining text
        if remaining.strip():
            parts.append(remaining.strip())
        
        return parts if parts else [text]

    async def _send_response_parts(self, message: discord.Message, response_parts: list, typing_delay: bool = True):
        """Send multiple response parts with human-like delays"""
        if not response_parts:
            return
        
        for i, part in enumerate(response_parts):
            if not part.strip():
                continue
            
            if typing_delay and i > 0:  # Add delay between parts (not for the first one)
                delay = await self._calculate_typing_delay(part)
                await asyncio.sleep(delay)
            
            try:
                await message.channel.send(part)
            except Exception as e:
                print(f"Error sending response part {i}: {e}")

    async def _calculate_typing_delay(self, text: str) -> float:
        import asyncio
        import random
        
        if not text:
            return 1.0

        # Base calculation: 80 WPM = ~400 characters per minute = ~10 characters per second
        base_chars_per_second = 10
        
        # Get Izumi's current mood and time personality from unified memory
        try:
            unified_memory = self.bot.unified_memory
            mood_data = unified_memory.get_daily_mood()
            time_personality = unified_memory.get_time_personality()
        except Exception:
            # Fallback to basic calculation if mood system unavailable
            base_delay = len(text) / base_chars_per_second
            variance = random.uniform(0.8, 1.2)
            return max(1.0, min(8.0, base_delay * variance))
        
        # Mood affects typing speed multiplier
        mood_multipliers = {
            "energetic": 0.75,    # 25% faster - quick energetic typing
            "excited": 0.70,      # 30% faster - very quick when excited
            "playful": 0.85,      # 15% faster - still quick but thoughtful
            "thoughtful": 1.15,   # 15% slower - pauses to think
            "sleepy": 1.25        # 50% slower - tired and sluggish
        }
        
        mood_multiplier = mood_multipliers.get(mood_data['current_mood'], 1.0)
        
        # Time of day affects typing speed
        time_multipliers = {
            "high": 0.85,         # Morning energy - faster typing
            "medium-high": 0.95,  # Afternoon - slightly faster
            "medium": 1.1,        # Evening - slightly slower
            "low": 1.4            # Late night - much slower, tired
        }
        
        time_multiplier = time_multipliers.get(time_personality['energy'], 1.0)
        
        # Energy level fine-tuning (0.0-1.0 scale affects speed)
        energy_level = mood_data.get('energy_level', 0.6)
        energy_multiplier = 1.4 - (energy_level * 0.6)  # High energy = faster, low energy = slower
        
        # Calculate adjusted typing speed
        final_multiplier = mood_multiplier * time_multiplier * energy_multiplier
        adjusted_chars_per_second = base_chars_per_second / final_multiplier
        
        # Calculate base delay
        base_delay = len(text) / adjusted_chars_per_second
        
        # Add human variance (Â±20%) but preserve mood influence
        variance = random.uniform(0.85, 1.15)
        
        # Apply bounds: minimum 0.5s for very short messages, maximum 12s for long tired responses
        if mood_data['current_mood'] == 'sleepy' and time_personality['energy'] == 'low':
            # Very tired Izumi can take longer
            delay = max(0.8, min(15.0, base_delay * variance))
        elif mood_data['current_mood'] in ['excited', 'energetic']:
            # Excited/energetic Izumi has tighter bounds
            delay = max(0.5, min(6.0, base_delay * variance))
        else:
            # Normal bounds
            delay = max(0.7, min(10.0, base_delay * variance))
        
        return delay
    
    async def _type_with_delay(self, channel, text: str):
        """Show typing indicator for human-like duration based on message length and mood"""
        import random
        
        delay = await self._calculate_typing_delay(text)
        
        # Get current mood for typing behavior variations
        try:
            mood_data = self.bot.unified_memory.get_daily_mood()
            current_mood = mood_data['current_mood']
        except Exception:
            current_mood = 'playful'  # Default fallback
        
        async with channel.typing():
            # Different typing patterns based on mood
            if current_mood == 'sleepy' and delay > 8:
                # Sleepy Izumi might pause mid-typing
                await asyncio.sleep(delay * 0.6)
                # Brief pause (like she dozed off for a second)
                await asyncio.sleep(random.uniform(0.5, 1.5))
                await asyncio.sleep(delay * 0.4)
            elif current_mood == 'thoughtful' and delay > 5:
                # Thoughtful Izumi has deliberate pauses
                await asyncio.sleep(delay * 0.7)
                await asyncio.sleep(random.uniform(0.3, 0.8))  # Thinking pause
                await asyncio.sleep(delay * 0.3)
            elif current_mood in ['excited', 'energetic'] and len(text) > 50:
                # Excited typing - quick bursts
                await asyncio.sleep(delay * 0.8)
                await asyncio.sleep(random.uniform(0.1, 0.3))  # Quick breath
                await asyncio.sleep(delay * 0.2)
            else:
                # Normal steady typing
                await asyncio.sleep(delay)
    
    async def _generate_response_with_fallback(self, user_id: int, channel_id: int, prompt: str, original_message: str, username: str = None) -> Optional[str]:
        """Generate response with model fallback and error handling using shared channel context"""
        
        # Track API usage
        self.daily_api_calls += 1
        self._save_api_usage_data()  # Save persistent data
        
        # Clear cache daily to prevent memory buildup
        current_time = time.time()
        if current_time - self.last_cache_clear > 86400:  # 24 hours
            # Save current day's data to historical before resetting
            import datetime
            historical_entry = {
                'date': datetime.datetime.fromtimestamp(self.last_cache_clear).strftime('%Y-%m-%d'),
                'api_calls': self.daily_api_calls,
                'quick_responses': self.daily_quick_responses,
                'total_interactions': self.daily_api_calls + self.daily_quick_responses
            }
            
            if 'historical_data' not in self.api_usage_data:
                self.api_usage_data['historical_data'] = []
            self.api_usage_data['historical_data'].append(historical_entry)
            
            # Keep only last 30 days of history
            if len(self.api_usage_data['historical_data']) > 30:
                self.api_usage_data['historical_data'] = self.api_usage_data['historical_data'][-30:]
            
            # Reset daily counters
            self.response_cache.clear()
            self.api_call_log.clear()
            self.daily_api_calls = 0
            self.daily_quick_responses = 0
            self.last_cache_clear = current_time
            
            # Save the reset data
            self._save_api_usage_data()
            print(f"ðŸ”„ Daily cache cleared. API calls and quick responses reset. Historical data saved.")
        
        # Log this API call
        self.api_call_log.append({
            'timestamp': current_time,
            'user_id': user_id,
            'channel_id': channel_id,
            'prompt_length': len(prompt)
        })
        
        # Keep only last 1000 API calls in log to prevent memory issues
        if len(self.api_call_log) > 1000:
            self.api_call_log = self.api_call_log[-1000:]
        
        print(f"ðŸ“Š API Call #{self.daily_api_calls} | Quick Responses: {self.daily_quick_responses}")
        
        session_data = self._get_or_create_session(channel_id)
        
        # Get personality enhancements
        mood_data = self.learning_engine.get_daily_mood()
        time_personality = self.learning_engine.get_time_personality()
        
        # Create enhanced user-aware prompt for shared conversation
        enhanced_prompt = self._enhance_prompt_with_personality(prompt, original_message, username, mood_data, time_personality)
        
        if username:
            user_aware_prompt = f"[User: {username}] {enhanced_prompt}"
        else:
            user_aware_prompt = f"[User ID: {user_id}] {enhanced_prompt}"
        
        MODEL_HIERARCHY = [
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-2.5-pro", 
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite",
            "gemini-1.5-flash",
            "gemini-1.5-flash-8b",
        ]
        
        for i, model_name in enumerate(MODEL_HIERARCHY):
            try:
                # Switch model if needed
                if session_data["model_index"] != i:
                    model = genai.GenerativeModel(
                        model_name=model_name,
                        safety_settings=self.safety_settings,
                        system_instruction=self.bot.system_prompt
                    )
                    history = session_data["chat"].history if session_data["chat"] else []
                    session_data["chat"] = model.start_chat(history=history)
                    session_data["model_index"] = i
                
                chat = session_data["chat"]
                response = await chat.send_message_async(user_aware_prompt)
                raw_response = response.text
                
                # Clean response to prevent context leakage
                cleaned_response = self._clean_response_output(raw_response)
                
                # Apply personality quirks and enhancements
                context_type = self._determine_context_type(original_message, cleaned_response)
                enhanced_response = self._apply_personality_quirks(cleaned_response, mood_data, context_type)
                
                return enhanced_response
                
            except Exception as e:
                error_str = str(e)
                
                if "PROHIBITED_CONTENT" in error_str:
                    return "Filtered."
                elif "429" in error_str or "quota" in error_str.lower():
                    print(f"âš ï¸ Rate limit on {model_name}, falling back...")
                    continue  # Try next model
                else:
                    print(f"Error with {model_name}: {e}")
                    continue
        
        # All models failed
        return None
    
    def _get_or_create_session(self, channel_id: int) -> Dict:
        """Get or create chat session for channel (shared by all users)"""
        if channel_id not in self.gemini_chat_sessions:
            self.gemini_chat_sessions[channel_id] = {
                "chat": None,
                "model_index": 0,
                "message_count": 0,
                "created_at": time.time()
            }
        
        session = self.gemini_chat_sessions[channel_id]
        
        # Create chat if doesn't exist
        if not session["chat"]:
            try:
                session["chat"] = self.gemini_model.start_chat()
            except Exception as e:
                print(f"Failed to create chat session: {e}")
        
        # Limit chat history - more aggressive trimming to prevent token buildup
        session["message_count"] += 1
        
        # First, check if we need to trim based on estimated token count (most important check)
        if self._trim_session_if_needed(session, channel_id):
            return session  # Session was reset, return early
        
        # Check if we need to trim the conversation history based on message count
        if session["message_count"] > self.chat_history_limit:
            # Reset session to limit context length
            try:
                session["chat"] = self.gemini_model.start_chat()
                session["message_count"] = 1
                print(f"Reset chat session for channel {channel_id} (hit message limit of {self.chat_history_limit})")
            except Exception as e:
                print(f"Failed to reset chat session: {e}")
        
        # Additional check: if session has been active for too long, also reset to prevent memory buildup
        elif session.get("created_at", 0) < time.time() - (2 * 60 * 60):  # 2 hours
            try:
                session["chat"] = self.gemini_model.start_chat()
                session["message_count"] = 1
                session["created_at"] = time.time()
                print(f"Reset chat session for channel {channel_id} (session too old - preventing memory buildup)")
            except Exception as e:
                print(f"Failed to reset aged chat session: {e}")
        
        return session
    
    def _estimate_session_tokens(self, session_data: Dict) -> int:
        """Estimate total tokens in the current chat session"""
        if not session_data.get("chat") or not hasattr(session_data["chat"], 'history'):
            return 0
        
        total_tokens = 0
        try:
            for content in session_data["chat"].history:
                if hasattr(content, 'parts'):
                    for part in content.parts:
                        if hasattr(part, 'text'):
                            # Rough estimation: ~4 characters per token
                            total_tokens += len(part.text) // 4
        except Exception as e:
            print(f"Error estimating session tokens: {e}")
            return 0
        
        return total_tokens
    
    def _trim_session_if_needed(self, session_data: Dict, channel_id: int) -> bool:
        """Check if session needs trimming based on estimated token count and trim if needed"""
        estimated_tokens = self._estimate_session_tokens(session_data)
        
        # If we're approaching token limits (conservatively set at 10,000 tokens), reset the session
        if estimated_tokens > 10000:
            try:
                session_data["chat"] = self.gemini_model.start_chat()
                session_data["message_count"] = 1
                session_data["created_at"] = time.time()
                print(f"Reset chat session for channel {channel_id} (estimated {estimated_tokens} tokens - preventing context overflow)")
                return True
            except Exception as e:
                print(f"Failed to reset session during token trimming: {e}")
        
        return False
    
    def _clean_response_output(self, response: str) -> str:
        """Clean AI response to prevent context/debug information from leaking through"""
        if not response:
            return response
        
        # Remove any context markers that might have leaked through
        import re
        
        # Remove internal context sections that leaked through
        response = re.sub(r'\[INTERNAL CONTEXT.*?\[END.*?\]', '', response, flags=re.DOTALL | re.IGNORECASE)
        
        # Remove legacy debug context sections
        response = re.sub(r'\[MEMORIES ABOUT THIS USER:.*?\]', '', response, flags=re.DOTALL | re.IGNORECASE)
        response = re.sub(r'\[ADDITIONAL DATA:.*?\]', '', response, flags=re.DOTALL | re.IGNORECASE)
        response = re.sub(r'\[EMOTIONAL CONTEXT.*?\]', '', response, flags=re.DOTALL | re.IGNORECASE)
        response = re.sub(r'ðŸ“… TODAY\'S DATE:.*?\n?', '', response, flags=re.IGNORECASE)
        
        # Remove specific debug patterns that commonly leak
        response = re.sub(r'Server level: \d+.*?\n?', '', response, flags=re.IGNORECASE)
        response = re.sub(r'Gacha (Coins|Cards): \d+.*?\n?', '', response, flags=re.IGNORECASE)
        response = re.sub(r'Messages Sent: \d+.*?\n?', '', response, flags=re.IGNORECASE)
        response = re.sub(r'Trust level: \d+(\.\d+)?/10.*?\n?', '', response, flags=re.IGNORECASE)
        response = re.sub(r'They prefer (short|long) messages\.*?\n?', '', response, flags=re.IGNORECASE)
        response = re.sub(r'XP: \d+.*?\n?', '', response, flags=re.IGNORECASE)
        
        # Remove any bracketed context markers
        response = re.sub(r'\[.*?(CONTEXT|DATA|INSTRUCTION|PERSONALITY|MEMORIES|GUIDANCE|INTERNAL).*?\]', '', response, flags=re.DOTALL | re.IGNORECASE)
        
        # Clean up any resulting multiple newlines or spaces
        response = re.sub(r'\n\s*\n+', '\n', response)
        response = re.sub(r'^\s+|\s+$', '', response)
        
        # Check for contextually inappropriate positive endings and remove them
        # Pattern: negative/confused content followed by positive phrase
        if any(word in response.lower() for word in ["confused", "mean", "rude", "annoying", "what", "why are you"]):
            # Remove inappropriate positive endings
            inappropriate_endings = ["that's awesome!", "so cool!", "amazing!", "love that!", "yesss!", "omg yes!"]
            for ending in inappropriate_endings:
                if response.lower().endswith(ending.lower()):
                    response = response[:-len(ending)].strip()
                    break
        
        return response
    
    async def _analyze_own_response(self, user_id: int, user_message: str, ai_response: str):
        """Analyze AI's own responses to improve learning"""
        # This could be used to learn from successful interactions
        # For example, track what types of responses get positive reactions
        user_id_str = str(user_id)
        
        # Ensure user exists in memory system
        if user_id_str not in self.learning_engine.memory_data['users']:
            self.learning_engine.memory_data['users'][user_id_str] = self.learning_engine._create_empty_user_profile()
        
        # Store response patterns for analysis using unified memory structure
        if 'ai_response_patterns' not in self.learning_engine.memory_data['users'][user_id_str]['learning_data']:
            self.learning_engine.memory_data['users'][user_id_str]['learning_data']['ai_response_patterns'] = []
        
        self.learning_engine.memory_data['users'][user_id_str]['learning_data']['ai_response_patterns'].append({
            'user_message': user_message[:100],  # Truncate for storage
            'ai_response': ai_response[:100],
            'timestamp': int(time.time()),
            'response_length': len(ai_response.split())
        })
        
        # Keep only recent response patterns
        patterns = self.learning_engine.memory_data['users'][user_id_str]['learning_data']['ai_response_patterns']
        if len(patterns) > 50:
            self.learning_engine.memory_data['users'][user_id_str]['learning_data']['ai_response_patterns'] = patterns[-50:]
    
    def _enhance_prompt_with_personality(self, prompt: str, original_message: str, username: str, mood_data: dict, time_personality: dict) -> str:
        """Enhance the prompt with current personality state and behavioral guidance"""
        enhancement_parts = []
        
        # Add mood context
        mood_desc = mood_data.get('mood_description', 'feeling normal')
        energy_level = mood_data.get('energy_level', 0.5)
        enhancement_parts.append(f"You are currently {mood_desc}")
        
        # Add time-based personality guidance
        time_style = time_personality.get('style', 'casual')
        greeting_options = time_personality.get('greeting_style', ['hey!'])
        enhancement_parts.append(f"Your current style should be {time_style}")
        
        # Add energy-based response guidance
        if energy_level > 0.8:
            enhancement_parts.append("Respond with high energy and enthusiasm")
        elif energy_level > 0.6:
            enhancement_parts.append("Respond with moderate energy and friendliness")
        elif energy_level > 0.3:
            enhancement_parts.append("Respond calmly and thoughtfully")
        else:
            enhancement_parts.append("Respond quietly and more subdued")
        
        # Add behavioral personality context
        enhancement_parts.append("Remember to use your natural speech patterns and occasional personality quirks")
        
        enhanced_prompt = f"{prompt}\n\nPERSONALITY CONTEXT: {' | '.join(enhancement_parts)}"
        return enhanced_prompt
    
    def _apply_personality_quirks(self, response_text: str, mood_data: dict, context_type: str = "general") -> str:
        """Apply personality quirks and speech patterns to the response"""
        if not response_text:
            return response_text
        
        # Get quirk decision
        quirk_decision = self.learning_engine.should_use_quirk(context_type, mood_data)
        
        if quirk_decision.get("use_quirk", False):
            quirk_content = quirk_decision.get("quirk_content", "")
            quirk_type = quirk_decision.get("quirk_type", "")
            
            if quirk_content and quirk_type:
                # Apply quirk based on type
                if quirk_type == "thinking":
                    response_text = f"{quirk_content} {response_text}"
                elif quirk_type == "enthusiasm":
                    response_text = f"{response_text} {quirk_content}"
                elif quirk_type == "agreement":
                    response_text = f"{quirk_content} {response_text}"
                elif quirk_type == "favorite_phrase":
                    # Only add favorite phrases if the response doesn't already express strong emotion
                    response_emotion_words = ["amazing", "awesome", "cool", "great", "love", "hate", "terrible", "awful", "annoying"]
                    if not any(word in response_text.lower() for word in response_emotion_words):
                        # Sometimes add to end, sometimes replace simple acknowledgments
                        if len(response_text.split()) <= 3:
                            response_text = quirk_content
                        else:
                            response_text = f"{response_text} {quirk_content}"
        
        # Apply occasional typos (very rarely)
        import random
        if random.random() < 0.05:  # 5% chance
            response_text = self._apply_occasional_typo(response_text)
        
        # Remove unnecessary periods from casual responses
        response_text = self._remove_casual_periods(response_text)
        
        return response_text
    
    def _apply_occasional_typo(self, text: str) -> str:
        """Occasionally add realistic typos that get corrected"""
        quirks = self.learning_engine.get_personality_quirks()
        typos = quirks.get("typing_habits", {}).get("occasional_typos", {})
        
        import random
        if typos and random.random() < 0.3:  # 30% chance when this method is called
            typo_word = random.choice(list(typos.keys()))
            correct_word = typos[typo_word]
            
            if correct_word.lower() in text.lower():
                # Make the typo then correct it
                text_with_typo = text.replace(correct_word, typo_word)
                return f"{text_with_typo}\n*{correct_word}"
        
        return text
    
    def _remove_casual_periods(self, text: str) -> str:
        """Remove unnecessary periods from casual responses to sound more natural"""
        if not text or len(text.strip()) == 0:
            return text
        
        text = text.strip()
        
        # Don't remove periods if:
        # 1. The response is long (more than 100 characters) - likely explanatory
        # 2. The response has multiple sentences (contains . followed by space and capital letter)
        # 3. The response ends with other punctuation (!, ?, etc.)
        # 4. The response contains formal indicators
        
        # Check if it's a long response
        if len(text) > 100:
            return text
        
        # Check if it has multiple sentences
        import re
        if re.search(r'\.\s+[A-Z]', text):
            return text
        
        # Check if it ends with other punctuation
        if text.endswith(('!', '?', '...', '~')):
            return text
        
        # Check for formal indicators that should keep periods
        formal_indicators = ['thank you', 'please', 'however', 'therefore', 'additionally', 'furthermore']
        text_lower = text.lower()
        if any(indicator in text_lower for indicator in formal_indicators):
            return text
        
        # Remove trailing period for casual responses
        if text.endswith('.'):
            return text[:-1]
        
        return text
    
    def _determine_context_type(self, original_message: str, response_text: str) -> str:
        """Determine the context type for quirk selection"""
        message_lower = original_message.lower()
        response_lower = response_text.lower()
        
        # Check for negative/confrontational contexts first (should NOT get positive quirks)
        negative_indicators = ["confused", "mean", "rude", "annoying", "stupid", "dumb", "hate", "angry", "mad", "wtf", "why are you"]
        if any(indicator in message_lower for indicator in negative_indicators):
            return "negative"
        
        # Check for disagreement/conflict contexts
        conflict_indicators = ["disagree", "wrong", "no", "nope", "not really", "i don't think", "that's not"]
        if any(indicator in message_lower for indicator in conflict_indicators):
            return "disagreement"
        
        # Check for genuine agreement context (only if both message and response show agreement)
        if any(word in response_lower for word in ["yes", "yeah", "exactly", "true", "agree", "right"]):
            agreement_indicators = ["yes", "yeah", "totally", "exactly", "true", "agree", "right", "definitely"]
            if any(indicator in message_lower for indicator in agreement_indicators):
                return "agreement"
        
        # Check for enthusiasm context (only for genuinely positive exchanges)
        enthusiasm_indicators = ["awesome", "cool", "amazing", "wow", "excited", "love", "amazing", "incredible"]
        if any(word in message_lower for word in enthusiasm_indicators) and any(word in response_lower for word in ["cool", "awesome", "amazing", "love", "excited"]):
            return "enthusiasm"
        
        # Check for thinking/question context
        if any(word in message_lower for word in ["think", "opinion", "what do you", "how do you", "why", "what"]):
            return "thinking"
        
        return "general"
    
    @commands.command(name='api_usage')
    @commands.has_permissions(administrator=True)
    async def api_usage(self, ctx):
        """Show API usage statistics and optimization info"""
        embed = discord.Embed(
            title="ðŸ“Š API Usage & Optimization Stats",
            color=discord.Color.green()
        )
        
        # Calculate time since last reset
        time_since_reset = time.time() - self.last_cache_clear
        hours_since_reset = time_since_reset / 3600
        
        # API Usage Stats
        total_interactions = self.daily_api_calls + self.daily_quick_responses
        if total_interactions > 0:
            api_percentage = (self.daily_api_calls / total_interactions) * 100
            quick_percentage = (self.daily_quick_responses / total_interactions) * 100
        else:
            api_percentage = quick_percentage = 0
        
        embed.add_field(
            name="ðŸ”¢ Daily Interactions",
            value=f"**{self.daily_api_calls}** API calls ({api_percentage:.1f}%)\n"
                  f"**{self.daily_quick_responses}** Quick responses ({quick_percentage:.1f}%)\n"
                  f"**{total_interactions}** Total interactions\n"
                  f"â° {hours_since_reset:.1f} hours since reset",
            inline=False
        )
        
        # Efficiency Stats
        if total_interactions > 0:
            api_savings = (self.daily_quick_responses / total_interactions) * 100
            embed.add_field(
                name="ðŸ’° API Savings",
                value=f"**{api_savings:.1f}%** of calls saved\n"
                      f"ðŸŽ¯ Quick responses working effectively",
                inline=True
            )
        
        # Cache Stats
        cache_size = len(self.response_cache)
        log_size = len(self.api_call_log)
        embed.add_field(
            name="ðŸ’¾ Cache & Logs",
            value=f"**{cache_size}** cached responses\n"
                  f"ðŸ“ **{log_size}** recent API calls logged\n"
                  f"ðŸ”„ Resets every 24 hours",
            inline=True
        )
        
        # Quick Response Stats (estimate based on patterns)
        quick_patterns = sum(len(data['patterns']) for data in self.quick_responses.values())
        embed.add_field(
            name="ðŸš€ Quick Responses",
            value=f"**{quick_patterns}** patterns available\n"
                  f"ðŸ’¡ Saves ~60-80% API calls",
            inline=True
        )
        
        # Performance Info
        if self.daily_api_calls > 0:
            avg_calls_per_hour = self.daily_api_calls / max(hours_since_reset, 0.1)
            projected_daily = avg_calls_per_hour * 24
            
            embed.add_field(
                name="ðŸ“ˆ Projections",
                value=f"**{avg_calls_per_hour:.1f}** calls/hour\n"
                      f"ðŸ“Š ~{projected_daily:.0f} projected daily",
                inline=True
            )
        
        # Cache efficiency
        embed.add_field(
            name="âš¡ Efficiency",
            value=f"ðŸŽ¯ Pattern matching: **Instant**\n"
                  f"ðŸ¤– API responses: **~2-5s**\n"
                  f"ðŸ’° Cost savings: **High**",
            inline=True
        )
        
        # Next reset time
        next_reset = self.last_cache_clear + 86400  # 24 hours
        next_reset_str = f"<t:{int(next_reset)}:R>"
        embed.add_field(
            name="ðŸ”„ Next Reset",
            value=f"{next_reset_str}",
            inline=True
        )
        
        embed.set_footer(text="Use this to monitor API usage and optimization effectiveness")
        await ctx.send(embed=embed)

    @commands.command(name='ai_stats')
    @commands.has_permissions(administrator=True)
    async def ai_stats(self, ctx):
        """Show AI learning statistics"""
        # Get learning data from unified memory structure
        memory_data = self.learning_engine.memory_data
        users_data = memory_data.get('users', {})
        
        embed = discord.Embed(
            title="ðŸ¤– AI Learning Statistics",
            color=discord.Color.blue()
        )
        
        # General stats - count users with various data types
        vocab_users = sum(1 for user_data in users_data.values() 
                         if user_data.get('learning_data', {}).get('vocabulary_trends'))
        sentiment_users = sum(1 for user_data in users_data.values() 
                             if user_data.get('learning_data', {}).get('sentiment_patterns'))
        relationship_users = sum(1 for user_data in users_data.values() 
                                if user_data.get('learning_data', {}).get('relationship_networks'))
        
        embed.add_field(
            name="ðŸ“Š Learning Coverage",
            value=f"ðŸ‘¥ Total Users: {len(users_data)}\n"
                  f"ðŸ“ Vocabulary Tracking: {vocab_users}\n"
                  f"ðŸ˜Š Sentiment Analysis: {sentiment_users}\n"
                  f"ðŸ¤ Relationship Networks: {relationship_users}",
            inline=False
        )
        
        active_sessions = len(self.gemini_chat_sessions)
        
        embed.add_field(name="ðŸ“± Active Sessions", value=active_sessions, inline=True)
        
        # Server culture stats - not implemented in unified memory yet
        # This could be added later if needed
        
        # Recent activity
        recent_activity = 0
        current_time = time.time()
        for session in self.gemini_chat_sessions.values():
            if current_time - session.get('created_at', 0) < 3600:  # Last hour
                recent_activity += 1
        
        embed.add_field(name="Active in last hour", value=recent_activity, inline=True)
        
        await ctx.send(embed=embed)
    
    @app_commands.command(name="ai_stats", description="Show AI learning statistics (Admin only)")
    @app_commands.default_permissions(administrator=True)
    async def slash_ai_stats(self, interaction: discord.Interaction):
        """Show AI learning statistics (slash command version)"""
        # Get learning data from unified memory structure
        memory_data = self.learning_engine.memory_data
        users_data = memory_data.get('users', {})
        
        embed = discord.Embed(
            title="ðŸ¤– AI Learning Statistics",
            color=discord.Color.blue()
        )
        
        # General stats - count users with various data types
        vocab_users = sum(1 for user_data in users_data.values() 
                         if user_data.get('learning_data', {}).get('vocabulary_trends'))
        sentiment_users = sum(1 for user_data in users_data.values() 
                             if user_data.get('learning_data', {}).get('sentiment_patterns'))
        relationship_users = sum(1 for user_data in users_data.values() 
                                if user_data.get('learning_data', {}).get('relationship_networks'))
        
        embed.add_field(
            name="ðŸ“Š Learning Coverage",
            value=f"ðŸ‘¥ Total Users: {len(users_data)}\n"
                  f"ðŸ“ Vocabulary Tracking: {vocab_users}\n"
                  f"ðŸ˜Š Sentiment Analysis: {sentiment_users}\n"
                  f"ðŸ¤ Relationship Networks: {relationship_users}",
            inline=False
        )
        
        active_sessions = len(self.gemini_chat_sessions)
        
        embed.add_field(name="ðŸ“± Active Sessions", value=active_sessions, inline=True)
        
        # Recent activity
        recent_activity = 0
        current_time = time.time()
        for session in self.gemini_chat_sessions.values():
            if current_time - session.get('created_at', 0) < 3600:  # Last hour
                recent_activity += 1
        
        embed.add_field(name="Active in last hour", value=recent_activity, inline=True)
        
        await interaction.response.send_message(embed=embed)
    
    @commands.command(name='ai_context')
    @commands.has_permissions(administrator=True)
    async def ai_context(self, ctx, user: discord.Member = None):
        """Show context data available for a user"""
        if not user:
            user = ctx.author
        
        summary = self.context_builder.get_context_summary(user.id, ctx.guild.id)
        
        embed = discord.Embed(
            title=f"ðŸ“Š Context Data for {user.display_name}",
            color=discord.Color.green()
        )
        
        # Data availability
        data_fields = {
            'vocabulary_data': 'Vocabulary Patterns',
            'relationship_data': 'Relationship Networks',
            'communication_style': 'Communication Style',
            'topic_interests': 'Topic Interests',
            'sentiment_data': 'Sentiment Patterns',
            'activity_patterns': 'Activity Patterns',
            'server_culture': 'Server Culture Data'
        }
        
        for key, name in data_fields.items():
            status = "âœ…" if summary.get(key, False) else "âŒ"
            embed.add_field(name=name, value=status, inline=True)
        
        # Detailed counts
        if summary.get('vocab_word_count'):
            embed.add_field(name="Words Tracked", value=summary['vocab_word_count'], inline=True)
        if summary.get('vocab_slang_count'):
            embed.add_field(name="Slang Terms", value=summary['vocab_slang_count'], inline=True)
        if summary.get('unique_mentions'):
            embed.add_field(name="Users Mentioned", value=summary['unique_mentions'], inline=True)
        
        await ctx.send(embed=embed)
    
    @commands.command(name='reset_chat')
    async def reset_chat(self, ctx, user: discord.Member = None):
        """Reset chat session for a user"""
        if not user:
            user = ctx.author
        
        # Only allow users to reset their own session or admins to reset anyone's
        if user != ctx.author and not ctx.author.guild_permissions.administrator:
            await ctx.send("You can only reset your own chat session!")
            return
        
        if user.id in self.gemini_chat_sessions:
            del self.gemini_chat_sessions[user.id]
            await ctx.send(f"âœ… Reset chat session for {user.display_name}")
        else:
            await ctx.send(f"No active chat session found for {user.display_name}")

    # Slash command versions of all prefix commands
    @app_commands.command(name="api_usage", description="Show API usage statistics and optimization info")
    @app_commands.describe()
    async def slash_api_usage(self, interaction: discord.Interaction):
        """Slash command version of api_usage"""
        if not interaction.user.guild_permissions.administrator:
            await interaction.response.send_message("âŒ This command requires administrator permissions.", ephemeral=True)
            return
        
        # Use the same logic as the prefix command
        embed = discord.Embed(
            title="ðŸ“Š API Usage & Optimization Stats",
            color=discord.Color.green()
        )
        
        # Calculate time since last reset
        time_since_reset = time.time() - self.last_cache_clear
        hours_since_reset = time_since_reset / 3600
        
        # API Usage Stats
        total_interactions = self.daily_api_calls + self.daily_quick_responses
        if total_interactions > 0:
            api_percentage = (self.daily_api_calls / total_interactions) * 100
            quick_percentage = (self.daily_quick_responses / total_interactions) * 100
        else:
            api_percentage = quick_percentage = 0
        
        embed.add_field(
            name="ðŸ”¢ Daily Interactions",
            value=f"**{self.daily_api_calls}** API calls ({api_percentage:.1f}%)\n"
                  f"**{self.daily_quick_responses}** Quick responses ({quick_percentage:.1f}%)\n"
                  f"**{total_interactions}** Total interactions\n"
                  f"â° {hours_since_reset:.1f} hours since reset",
            inline=False
        )
        
        # Efficiency Stats
        if total_interactions > 0:
            api_savings = (self.daily_quick_responses / total_interactions) * 100
            embed.add_field(
                name="ðŸ’° API Savings",
                value=f"**{api_savings:.1f}%** of calls saved\n"
                      f"ðŸŽ¯ Quick responses working effectively",
                inline=True
            )
        
        # Cache Stats
        cache_size = len(self.response_cache)
        log_size = len(self.api_call_log)
        embed.add_field(
            name="ðŸ’¾ Cache & Logs",
            value=f"**{cache_size}** cached responses\n"
                  f"ðŸ“ **{log_size}** recent API calls logged\n"
                  f"ðŸ”„ Resets every 24 hours",
            inline=True
        )
        
        # Quick Response Stats
        quick_patterns = sum(len(data['patterns']) for data in self.quick_responses.values())
        embed.add_field(
            name="ðŸš€ Quick Responses",
            value=f"**{quick_patterns}** patterns available\n"
                  f"ðŸ’¡ Saves ~60-80% API calls",
            inline=True
        )
        
        # Performance Info
        if self.daily_api_calls > 0:
            avg_calls_per_hour = self.daily_api_calls / max(hours_since_reset, 0.1)
            projected_daily = avg_calls_per_hour * 24
            
            embed.add_field(
                name="ðŸ“ˆ Projections",
                value=f"**{avg_calls_per_hour:.1f}** calls/hour\n"
                      f"ðŸ“Š ~{projected_daily:.0f} projected daily",
                inline=True
            )
        
        # Cache efficiency
        embed.add_field(
            name="âš¡ Efficiency",
            value=f"ðŸŽ¯ Pattern matching: **Instant**\n"
                  f"ðŸ¤– API responses: **~2-5s**\n"
                  f"ðŸ’° Cost savings: **High**",
            inline=True
        )
        
        # Next reset time
        next_reset = self.last_cache_clear + 86400  # 24 hours
        next_reset_str = f"<t:{int(next_reset)}:R>"
        embed.add_field(
            name="ðŸ”„ Next Reset",
            value=f"{next_reset_str}",
            inline=True
        )
        
        embed.set_footer(text="Use this to monitor API usage and optimization effectiveness")
        await interaction.response.send_message(embed=embed)

    @app_commands.command(name="ai_stats", description="Show AI learning statistics")
    @app_commands.describe()
    async def slash_ai_stats(self, interaction: discord.Interaction):
        """Slash command version of ai_stats"""
        if not interaction.user.guild_permissions.administrator:
            await interaction.response.send_message("âŒ This command requires administrator permissions.", ephemeral=True)
            return
        
        # Get learning data from unified memory structure
        memory_data = self.learning_engine.memory_data
        users_data = memory_data.get('users', {})
        
        embed = discord.Embed(
            title="ðŸ¤– AI Learning Statistics",
            color=discord.Color.blue()
        )
        
        # General stats - count users with various data types
        vocab_users = sum(1 for user_data in users_data.values() 
                         if user_data.get('learning_data', {}).get('vocabulary_trends'))
        sentiment_users = sum(1 for user_data in users_data.values() 
                             if user_data.get('learning_data', {}).get('sentiment_patterns'))
        relationship_users = sum(1 for user_data in users_data.values() 
                                if user_data.get('learning_data', {}).get('relationship_networks'))
        
        embed.add_field(
            name="ðŸ“Š Learning Coverage",
            value=f"**{len(users_data)}** total users tracked\n"
                  f"**{vocab_users}** users with vocabulary data\n"
                  f"**{sentiment_users}** users with sentiment data\n"
                  f"**{relationship_users}** users with relationship data",
            inline=True
        )
        
        # Memory stats
        total_memories = memory_data.get('daily_memories', {})
        recent_memories = len([m for m in total_memories.values() if m.get('timestamp', 0) > time.time() - 604800])  # Last week
        
        embed.add_field(
            name="ðŸ§  Memory System",
            value=f"**{len(total_memories)}** total daily memories\n"
                  f"**{recent_memories}** memories from last week\n"
                  f"**Active** unified memory system",
            inline=True
        )
        
        # Current mood
        try:
            current_mood = self.learning_engine.get_daily_mood()
            time_personality = self.learning_engine.get_time_personality()
            
            embed.add_field(
                name="ðŸŽ­ Current State",
                value=f"**Mood:** {current_mood['current_mood'].title()}\n"
                      f"**Energy:** {time_personality['energy'].title()}\n"
                      f"**Time:** {time_personality['time_period']}\n"
                      f"**Level:** {current_mood.get('energy_level', 0.5):.1f}/1.0",
                inline=True
            )
        except Exception as e:
            embed.add_field(
                name="ðŸŽ­ Current State",
                value="Unable to retrieve mood data",
                inline=True
            )
        
        embed.set_footer(text="Learning system continuously adapts to user interactions")
        await interaction.response.send_message(embed=embed)

    @app_commands.command(name="ai_memory_status", description="Check AI memory and context usage")
    @app_commands.describe()
    async def slash_ai_memory_status(self, interaction: discord.Interaction):
        """Check current AI memory usage and context status"""
        if not interaction.user.guild_permissions.administrator:
            await interaction.response.send_message("âŒ This command requires administrator permissions.", ephemeral=True)
            return
        
        embed = discord.Embed(
            title="ðŸ§  AI Memory Status",
            description="Current memory usage and context statistics",
            color=discord.Color.blue()
        )
        
        # Chat sessions info
        active_sessions = len(self.gemini_chat_sessions)
        embed.add_field(
            name="ðŸ’¬ Active Chat Sessions",
            value=f"**{active_sessions}** channels with active conversations",
            inline=True
        )
        
        # Session details
        total_messages = sum(session.get("message_count", 0) for session in self.gemini_chat_sessions.values())
        embed.add_field(
            name="ðŸ“Š Session Statistics",
            value=f"**{total_messages}** total messages across all sessions\n"
                  f"**{self.chat_history_limit}** message limit per session\n"
                  f"**10,000** token limit per session",
            inline=True
        )
        
        # Check specific session for this channel
        channel_session = self.gemini_chat_sessions.get(interaction.channel.id)
        if channel_session:
            estimated_tokens = self._estimate_session_tokens(channel_session)
            embed.add_field(
                name="ðŸ” Current Channel",
                value=f"**{channel_session.get('message_count', 0)}** messages\n"
                      f"**~{estimated_tokens}** estimated tokens\n"
                      f"**{time.time() - channel_session.get('created_at', time.time()):.0f}s** session age",
                inline=True
            )
        else:
            embed.add_field(
                name="ðŸ” Current Channel",
                value="No active session in this channel",
                inline=True
            )
        
        # Context system
        embed.add_field(
            name="ðŸ“ Context System",
            value=f"**{self.context_builder.max_context_tokens}** max context tokens\n"
                  f"**50** recent messages per channel\n"
                  f"**2 hours** max session age",
            inline=True
        )
        
        # Memory management
        embed.add_field(
            name="ðŸ”§ Memory Management",
            value="**Automatic cleanup:** Every 6 hours\n"
                  "**Session reset:** Token/age limits\n"
                  "**Data trimming:** Continuous",
            inline=True
        )
        
        embed.set_footer(text="Memory management prevents token buildup and maintains performance")
        await interaction.response.send_message(embed=embed)

    @app_commands.command(name="ai_reset_context", description="Manually reset AI context for this channel")
    @app_commands.describe()
    async def slash_ai_reset_context(self, interaction: discord.Interaction):
        """Manually reset AI conversation context for the current channel"""
        if not interaction.user.guild_permissions.administrator:
            await interaction.response.send_message("âŒ This command requires administrator permissions.", ephemeral=True)
            return
        
        channel_id = interaction.channel.id
        
        # Reset chat session if it exists
        if channel_id in self.gemini_chat_sessions:
            try:
                self.gemini_chat_sessions[channel_id] = {
                    "chat": self.gemini_model.start_chat(),
                    "model_index": 0,
                    "message_count": 0,
                    "created_at": time.time()
                }
                
                # Also clear recent messages from unified memory
                if hasattr(self.learning_engine, 'recent_messages') and channel_id in self.learning_engine.recent_messages:
                    self.learning_engine.recent_messages[channel_id].clear()
                
                await interaction.response.send_message("âœ… **AI context reset successfully!**\n"
                                                       "The conversation history for this channel has been cleared.", 
                                                       ephemeral=True)
            except Exception as e:
                await interaction.response.send_message(f"âŒ **Error resetting context:** {str(e)}", ephemeral=True)
        else:
            await interaction.response.send_message("â„¹ï¸ **No active context to reset**\n"
                                                   "This channel doesn't have an active AI conversation session.", 
                                                   ephemeral=True)

    @app_commands.command(name="ai_context", description="Show context building information")
    @app_commands.describe()
    async def slash_ai_context(self, interaction: discord.Interaction):
        """Slash command version of ai_context"""
        if not interaction.user.guild_permissions.administrator:
            await interaction.response.send_message("âŒ This command requires administrator permissions.", ephemeral=True)
            return
        
        embed = discord.Embed(
            title="ðŸ§  AI Context System",
            color=discord.Color.purple()
        )
        
        # Context builder stats
        try:
            context = self.context_builder.build_smart_context(
                user_id=interaction.user.id,
                guild_id=interaction.guild.id,
                current_message="test context",
                channel_id=interaction.channel.id
            )
            
            context_length = len(context)
            
            embed.add_field(
                name="ðŸ“ Context Building",
                value=f"**{context_length}** characters in current context\n"
                      f"**Active** smart context system\n"
                      f"**Dynamic** personality integration",
                inline=True
            )
        except Exception as e:
            embed.add_field(
                name="ðŸ“ Context Building",
                value="Context system active but unable to generate sample",
                inline=True
            )
        
        # Session info
        active_sessions = len(self.gemini_chat_sessions)
        embed.add_field(
            name="ðŸ’¬ Active Sessions",
            value=f"**{active_sessions}** active chat sessions\n"
                  f"**{self.chat_history_limit}** message history limit per channel",
            inline=True
        )
        
        # Participation tracking
        active_channels = len(self.participation_tracker)
        embed.add_field(
            name="ðŸŽ­ Conversation Tracking",
            value=f"**{active_channels}** channels being tracked\n"
                  f"**Smart** conversation participation\n"
                  f"**Context-aware** responses",
            inline=True
        )
        
        embed.set_footer(text="Context system builds comprehensive understanding for each interaction")
        await interaction.response.send_message(embed=embed)

    @app_commands.command(name="reset_chat", description="Reset chat session for a user")
    @app_commands.describe(user="The user whose chat session to reset (leave empty for yourself)")
    async def slash_reset_chat(self, interaction: discord.Interaction, user: discord.Member = None):
        """Slash command version of reset_chat"""
        if not user:
            user = interaction.user
        
        # Only allow users to reset their own session or admins to reset anyone's
        if user != interaction.user and not interaction.user.guild_permissions.administrator:
            await interaction.response.send_message("âŒ You can only reset your own chat session!", ephemeral=True)
            return
        
        if user.id in self.gemini_chat_sessions:
            del self.gemini_chat_sessions[user.id]
            await interaction.response.send_message(f"âœ… Reset chat session for {user.display_name}")
        else:
            await interaction.response.send_message(f"No active chat session found for {user.display_name}")

    @tasks.loop(minutes=10)
    async def save_learning_data_task(self):
        """Periodically save unified memory data"""
        try:
            await self.learning_engine.auto_save()
            # print("ðŸ§  Auto-saved unified memory data")
        except Exception as e:
            print(f"Error saving unified memory data: {e}")
    
    @tasks.loop(hours=2)  # More frequent cleanup to prevent memory buildup
    async def cleanup_learning_data_task(self):
        """Clean up old learning data and manage memory usage"""
        try:
            await self._cleanup_old_data()
            await self._proactive_session_cleanup()
            print("ðŸ§¹ Cleaned up old learning data and managed sessions")
        except Exception as e:
            print(f"Error cleaning up learning data: {e}")
    
    async def _proactive_session_cleanup(self):
        """Proactively clean up chat sessions that might be consuming too much memory"""
        current_time = time.time()
        sessions_cleaned = 0
        
        for channel_id, session in list(self.gemini_chat_sessions.items()):
            reset_needed = False
            reason = ""
            
            # Check token usage
            estimated_tokens = self._estimate_session_tokens(session)
            if estimated_tokens > 8000:  # Conservative limit
                reset_needed = True
                reason = f"high token usage ({estimated_tokens})"
            
            # Check message count
            elif session.get("message_count", 0) > 75:  # 75% of limit
                reset_needed = True
                reason = f"high message count ({session.get('message_count', 0)})"
            
            # Check session age
            elif current_time - session.get('created_at', current_time) > (90 * 60):  # 1.5 hours
                reset_needed = True
                reason = f"session age ({(current_time - session.get('created_at', current_time)) // 60:.0f} minutes)"
            
            if reset_needed:
                try:
                    session["chat"] = self.gemini_model.start_chat()
                    session["message_count"] = 1
                    session["created_at"] = current_time
                    sessions_cleaned += 1
                    print(f"ðŸ”„ Proactively reset session for channel {channel_id} (reason: {reason})")
                except Exception as e:
                    print(f"Error resetting session {channel_id}: {e}")
        
        if sessions_cleaned > 0:
            print(f"ðŸ§¹ Proactively cleaned {sessions_cleaned} chat sessions to prevent memory buildup")
    
    async def _cleanup_old_data(self):
        """Remove old and excessive learning data"""
        memory_data = self.learning_engine.memory_data
        current_time = time.time()
        cutoff_time = current_time - (30 * 24 * 60 * 60)  # 30 days
        
        # Clean up old chat sessions
        sessions_to_remove = []
        for user_id, session in self.gemini_chat_sessions.items():
            if current_time - session.get('created_at', 0) > 4 * 60 * 60:  # 4 hours - more aggressive cleanup
                sessions_to_remove.append(user_id)
        
        for user_id in sessions_to_remove:
            del self.gemini_chat_sessions[user_id]
        
        # Clean up learning data for inactive users in unified memory
        if 'users' in memory_data:
            for user_id_str in list(memory_data['users'].keys()):
                try:
                    user_id = int(user_id_str)
                    user_data = memory_data['users'][user_id_str]
                    last_interaction = user_data.get('activity', {}).get('last_interaction', 0)
                    
                    if last_interaction < cutoff_time:
                        # Trim learning data for inactive users
                        learning_data = user_data.get('learning_data', {})
                        
                        if 'vocabulary' in learning_data:
                            vocab = learning_data['vocabulary']
                            # Keep only top items
                            if len(vocab.get('word_frequency', {})) > 30:
                                sorted_words = sorted(vocab['word_frequency'].items(), key=lambda x: x[1], reverse=True)
                                vocab['word_frequency'] = dict(sorted_words[:30])
                            
                            vocab['message_lengths'] = vocab.get('message_lengths', [])[-50:]
                            vocab['question_patterns'] = vocab.get('question_patterns', [])[-10:]
                            vocab['exclamation_patterns'] = vocab.get('exclamation_patterns', [])[-10:]
                except:
                    continue
    
    def cog_unload(self):
        """Clean shutdown"""
        self.save_learning_data_task.cancel()
        self.cleanup_learning_data_task.cancel()
        
        # Save data before shutdown
        try:
            self.learning_engine.save_unified_data()
            self._save_api_usage_data()  # Save API usage data
            print("ðŸ’¾ Saved API usage data before shutdown")
        except Exception as e:
            print(f"Error saving data during shutdown: {e}")

async def setup(bot):
    await bot.add_cog(IzumiAI(bot))
